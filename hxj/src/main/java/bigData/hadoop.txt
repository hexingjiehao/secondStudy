1.å­¦ä¹ hadoopçŸ¥è¯†ç‚¹ï¼š
    1.å®˜ç½‘ï¼šhttp://hadoop.apache.org
    2.æ¦‚è¿°ï¼šhadoopæœ‰ä¸¤ç§ç‰ˆæœ¬ï¼š2.xå’Œ3.xã€‚2019å¹´11æœˆ6æ—¥æœ€æ–°çš„ç¨³å®šç‰ˆæœ¬æ˜¯2.10å’Œ3.1.3ã€‚æœ€æ–°çš„ç‰ˆæœ¬æ˜¯3.2.1ã€‚
           Apache Hadoopè½¯ä»¶åº“æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œå®ƒå…è®¸ä½¿ç”¨ç®€å•çš„ç¼–ç¨‹æ¨¡å‹è·¨è®¡ç®—æœºé›†ç¾¤å¯¹å¤§å‹æ•°æ®é›†è¿›è¡Œåˆ†å¸ƒå¼å¤„ç†ã€‚ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿
           å®ƒè¢«è®¾è®¡æˆä»å•ä¸ªæœåŠ¡å™¨æ‰©å±•åˆ°æ•°åƒå°æœºå™¨ï¼Œæ¯å°æœºå™¨éƒ½æä¾›æœ¬åœ°è®¡ç®—å’Œå­˜å‚¨ã€‚
           åº“æœ¬èº«çš„è®¾è®¡ç›®çš„æ˜¯åœ¨åº”ç”¨å±‚æ£€æµ‹å’Œå¤„ç†æ•…éšœï¼Œè€Œä¸æ˜¯ä¾èµ–ç¡¬ä»¶æ¥æä¾›é«˜å¯ç”¨æ€§ï¼Œå› æ­¤åœ¨è®¡ç®—æœºé›†ç¾¤ä¹‹ä¸Šæä¾›é«˜å¯ç”¨æ€§æœåŠ¡ï¼Œè€Œæ¯ä¸ªé›†ç¾¤éƒ½å¯èƒ½å®¹æ˜“å‡ºç°æ•…éšœã€‚
    3.è¯´æ˜æ–‡æ¡£çš„é¦–é¡µï¼šhttps://hadoop.apache.org/docs/stable/index.html
        è¿™æ˜¯ä¸€ä¸ªç®€é™‹çš„è¯´æ˜é¡µé¢ï¼Œå·¦è¾¹æ˜¯å¯¼èˆªæ 

    ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ï¼šHadoop=core + HDFS + MapReduce + YARN + Submarine + Tools

2.ç³»ç»Ÿçš„å­¦ä¹ hadoopçš„çŸ¥è¯†ç‚¹ï¼šã€æ€»è€Œè¨€ä¹‹å°±æ˜¯ä¸€ç³»åˆ—çš„é…ç½®æ–‡ä»¶çš„å†…å®¹ä¿®æ”¹ï¼ï¼ï¼ã€‘
    1.ç»¼è¿°:
        1.1 å•èŠ‚ç‚¹è®¾ç½®:{ubuntu}
            å‡†å¤‡å®‰è£…java, ssh å’Œpdsh
              $ sudo apt-get install ssh
              $ sudo apt-get install pdsh
            å¯åŠ¨hadoop
                æ‰“å¼€æ–‡ä»¶/etc/hadoop/hadoop-env.sh
                  export JAVA_HOME=/usr/java/latest
                $ bin/hadoop
            ç‹¬ç«‹å¯åŠ¨ï¼šğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿
                $ mkdir input
                $ cp etc/hadoop/*.xml input
                $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep input output 'dfs[a-z.]+'
                $ cat output/*
            ä¼ªåˆ†å¸ƒå¯åŠ¨ï¼š
                é…ç½®æ–‡ä»¶ï¼š
                    etc/hadoop/core-site.xml:
                    <configuration>
                        <property>
                            <name>fs.defaultFS</name>
                            <value>hdfs://localhost:9000</value>
                        </property>
                    </configuration>

                    etc/hadoop/hdfs-site.xml:
                    <configuration>
                        <property>
                            <name>dfs.replication</name>
                            <value>1</value>
                        </property>
                    </configuration>
                è®¾ç½®æ— éœ€å‚æ•°çš„sshè¿æ¥ï¼š
                    $ ssh localhost   //æœ€ç»ˆæ£€æŸ¥æ•ˆæœ

                    $ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
                    $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
                    $ chmod 0600 ~/.ssh/authorized_keys
                åœ¨æœ¬åœ°æ‰§è¡ŒMapReduce jobï¼šğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿
                    1.æ ¼å¼åŒ–æ–‡ä»¶ç³»ç»Ÿï¼š
                        $ bin/hdfs namenode -format
                    2.å¯åŠ¨å‘½åèŠ‚ç‚¹å’Œæ•°æ®èŠ‚ç‚¹è¿›ç¨‹
                        $ sbin/start-dfs.sh
                        hadoopçš„æ—¥å¿—è¾“å‡ºé»˜è®¤å†™å…¥æ–‡ä»¶ $HADOOP_LOG_DIR æˆ–è€… $HADOOP_HOME/logs ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿
                    3.æµè§ˆå‘½åèŠ‚ç‚¹çš„ç½‘é¡µæ¥å£
                        http://localhost:9870/
                    4.ä½¿hdfsæ‰§è¡ŒMapReduce job
                        $ bin/hdfs dfs -mkdir /user
                        $ bin/hdfs dfs -mkdir /user/<username>
                    5.å°†è¾“å…¥æ–‡ä»¶å¤åˆ¶åˆ°åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ
                        $ bin/hdfs dfs -mkdir input
                        $ bin/hdfs dfs -put etc/hadoop/*.xml input
                    6.è¿è¡ŒMapReduce jobä¸¾ä¾‹ï¼š
                        $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep input output 'dfs[a-z.]+'
                    7.å°†è¾“å‡ºæ–‡ä»¶å¤åˆ¶åˆ°æœ¬åœ°å¹¶æ£€æŸ¥å®ƒä»¬ï¼š
                        $ bin/hdfs dfs -get output output
                        $ cat output/*
                        åœ¨åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿæ£€æŸ¥è¾“å‡ºæ–‡ä»¶ï¼š$ bin/hdfs dfs -cat output/*
                    8.ç»“æŸhadoop
                        $ sbin/stop-dfs.sh
                åœ¨YARNä¸Šä»¥ä¼ªåˆ†å¸ƒå¼æ¨¡å¼è¿è¡ŒMapReduceä½œä¸š:
                    1.å·²ç»æ‰§è¡Œå®Œä»¥ä¸Š4æ­¥
                    2.é…ç½®å‚æ•°ï¼š
                        etc/hadoop/mapred-site.xml:
                        <configuration>
                            <property>
                                <name>mapreduce.framework.name</name>
                                <value>yarn</value>
                            </property>
                            <property>
                                <name>mapreduce.application.classpath</name>
                                <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
                            </property>
                        </configuration>

                        etc/hadoop/yarn-site.xml:
                        <configuration>
                            <property>
                                <name>yarn.nodemanager.aux-services</name>
                                <value>mapreduce_shuffle</value>
                            </property>
                            <property>
                                <name>yarn.nodemanager.env-whitelist</name>
                                <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
                            </property>
                        </configuration>
                    3.å¯åŠ¨èµ„æºç®¡ç†å™¨å’ŒèŠ‚ç‚¹ç®¡ç†å™¨è¿›ç¨‹
                        $ sbin/start-yarn.sh
                    4.æµè§ˆèµ„æºç®¡ç†å™¨çš„ç½‘é¡µæ¥å£
                        http://localhost:8088/
                    5.è¿è¡ŒMapReduce job
                        $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep input output 'dfs[a-z.]+'
                    6.ç»“æŸhadoop
                        $ sbin/stop-dfs.sh
        1.2 é›†ç¾¤è®¾ç½®
            åœ¨ä¸å®‰å…¨æ¨¡å¼ä¸‹é…ç½®hadoop
                åªè¯»é»˜è®¤é…ç½®ï¼š
                   core-default.xml
                   hdfs-default.xml
                   yarn-default.xml
                   mapred-default.xml
                ç‰¹å®šç«™ç‚¹é…ç½®ï¼š
                   etc/hadoop/core-site.xml
                   etc/hadoop/hdfs-site.xml
                   etc/hadoop/yarn-site.xml
                   etc/hadoop/mapred-site.xml
                å…¶ä»–ï¼š
                   etc/hadoop/hadoop-env.sh
                   etc/hadoop/yarn-env.sh
                HDFSå®ˆæŠ¤è¿›ç¨‹æ˜¯å‘½åèŠ‚ç‚¹ï¼Œç¬¬äºŒå‘½åèŠ‚ç‚¹ï¼Œæ•°æ®èŠ‚ç‚¹
                YARNå®ˆæŠ¤è¿›ç¨‹èµ„æºç®¡ç†å™¨ï¼ŒèŠ‚ç‚¹ç®¡ç†å™¨ï¼Œwebåº”ç”¨ä»£ç†å™¨
                é…ç½®hadoopå®ˆæŠ¤è¿›ç¨‹çš„ç¯å¢ƒå˜é‡ï¼š
                    NameNode	                    HDFS_NAMENODE_OPTS
                    DataNode	                    HDFS_DATANODE_OPTS
                    Secondary NameNode	            HDFS_SECONDARYNAMENODE_OPTS
                    ResourceManager	                YARN_RESOURCEMANAGER_OPTS
                    NodeManager	                    YARN_NODEMANAGER_OPTS
                    WebAppProxy	                    YARN_PROXYSERVER_OPTS
                    Map Reduce Job History Server	MAPRED_HISTORYSERVER_OPTS

                    ä¸¾ä¾‹ï¼šexport HDFS_NAMENODE_OPTS="-XX:+UseParallelGC -Xmx4g"
                    HADOOP_HOME=/path/to/hadoop
                    export HADOOP_HOME
                é‡è¦å‚æ•°çš„é…ç½®ï¼šğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿
                    fs.defaultFS  hdfs://host:port/
                å¯ä»¥è‡ªåŠ¨æ£€æŸ¥é›†ç¾¤èŠ‚ç‚¹çš„å¥åº·ã€‚é€šè¿‡è„šæœ¬æ‰§è¡Œ
                hadoopé›†ç¾¤å¯åŠ¨ï¼šéœ€è¦å…ˆå¯åŠ¨ HDFS å’Œ YARNé›†ç¾¤
                    å¯åŠ¨hadoopï¼š
                        [hdfs]$ $HADOOP_HOME/bin/hdfs namenode -format <cluster_name>
                        [hdfs]$ $HADOOP_HOME/bin/hdfs --daemon start namenode
                        [hdfs]$ $HADOOP_HOME/bin/hdfs --daemon start datanode
                        [hdfs]$ $HADOOP_HOME/sbin/start-dfs.sh
                        [yarn]$ $HADOOP_HOME/bin/yarn --daemon start resourcemanager
                        [yarn]$ $HADOOP_HOME/bin/yarn --daemon start nodemanager
                        [yarn]$ $HADOOP_HOME/bin/yarn --daemon start proxyserver
                        [yarn]$ $HADOOP_HOME/sbin/start-yarn.sh
                        [mapred]$ $HADOOP_HOME/bin/mapred --daemon start historyserver
                    å…³é—­hadoop
                        [hdfs]$ $HADOOP_HOME/bin/hdfs --daemon stop namenode
                        [hdfs]$ $HADOOP_HOME/bin/hdfs --daemon stop datanode
                        [hdfs]$ $HADOOP_HOME/sbin/stop-dfs.sh
                        [yarn]$ $HADOOP_HOME/bin/yarn --daemon stop resourcemanager
                        [yarn]$ $HADOOP_HOME/bin/yarn --daemon stop nodemanager
                        [yarn]$ $HADOOP_HOME/sbin/stop-yarn.sh
                        [yarn]$ $HADOOP_HOME/bin/yarn stop proxyserver
                        [mapred]$ $HADOOP_HOME/bin/mapred --daemon stop historyserver
                    ç½‘é¡µæ¥å£ï¼š
                        å‘½åèŠ‚ç‚¹ï¼šhttp://localhost:9870
                        èµ„æºç®¡ç†å™¨ï¼šhttp://localhost:8088
                        MapReduce jobå†å²æœåŠ¡å™¨ï¼šhttp://localhost:19888
    2.å…¬å…±æ ¸å¿ƒ:
        2.1 å°å‹é›†ç¾¤ ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿
                Hadoop tarball: $ mvn clean install -DskipTests
                                $ mvn package -Pdist -Dtar -DskipTests -Dmaven.javadoc.skip
                è¿è¡Œæœ€å°é›†ç¾¤ï¼š
                    $ bin/mapred minicluster -rmport RM_PORT -jhsport JHS_PORT
        2.2 æœ¬åœ°åº“æŒ‡å—ï¼š
                æ„å»ºåŸç”Ÿhadoopåº“ï¼š
                    $ mvn package -Pdist,native -DskipTests -Dtar
                    $ hadoop-dist/target/hadoop-3.2.1/lib/native
        2.3 å®‰å…¨æ¨¡å‹ä¸‹çš„hadoop: åŒ…æ‹¬èº«ä»½éªŒè¯ã€æœåŠ¡çº§åˆ«æˆæƒã€Webæ§åˆ¶å°èº«ä»½éªŒè¯å’Œæ•°æ®æœºå¯†æ€§
        2.4 æœåŠ¡å±‚çº§æˆæƒï¼š é»˜è®¤æ˜¯ç¦ç”¨æœåŠ¡çº§æˆæƒ
                åˆ·æ–°æˆæƒçº§åˆ«é…ç½®ï¼š
                    $ bin/hdfs dfsadmin -refreshServiceAcl
                    $ bin/yarn rmadmin -refreshServiceAcl
                é…ç½®ä¸¾ä¾‹ï¼š<property>
                          <name>security.job.client.protocol.acl</name>
                          <value>alice,bob mapreduce</value>
                        </property>
        2.5 hadoop KMSï¼šç§˜é’¥ç®¡ç†æœåŠ¡å™¨--æ–‡æ¡£é›†ã€‚é€šè¿‡rest è¯·æ±‚è®¿é—®ç»“æœ
                å¯åŠ¨ï¼šhadoop-3.2.1 $ hadoop --daemon start kms
    3.HDFSï¼šé«˜å¯ç”¨åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ
        3.1 ç¼–è¾‘æŸ¥çœ‹å™¨ï¼š
                è¾“å…¥æ–‡ä»¶ç”±è¾“å‡ºæ–‡ä»¶è¿›è¡ŒæŸ¥çœ‹ï¼š-iè¾“å…¥ï¼Œ-oè¾“å‡º
                    bash$ bin/hdfs oev -p xml -i edits -o edits.xml
                    bash$ bin/hdfs oev -i edits -o edits.xml
                    bash$ bin/hdfs oev -p binary -i edits.xml -o edits
                    bash$ bin/hdfs oev -p stats -i edits -o edits.stats
    4.MapReduce: å¤§æ•°æ®æ‹†åˆ†å¹¶è¡Œè¿ç®—
        4.1 æ•™ç¨‹:
                è¾“å…¥è¾“å‡ºä»¥é”®å€¼å¯¹å½¢å¼<k,v>
                ä»£ç ä¸¾ä¾‹ï¼š
                    import java.io.IOException;
                    import java.util.StringTokenizer;
                    import org.apache.hadoop.conf.Configuration;
                    import org.apache.hadoop.fs.Path;
                    import org.apache.hadoop.io.IntWritable;
                    import org.apache.hadoop.io.Text;
                    import org.apache.hadoop.mapreduce.Job;
                    import org.apache.hadoop.mapreduce.Mapper;
                    import org.apache.hadoop.mapreduce.Reducer;
                    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
                    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

                    public class WordCount {
                      public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable>{
                        private final static IntWritable one = new IntWritable(1);
                        private Text word = new Text();
                        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
                          StringTokenizer itr = new StringTokenizer(value.toString());
                          while (itr.hasMoreTokens()) {
                            word.set(itr.nextToken());
                            context.write(word, one);
                          }
                        }
                      }

                      public static class IntSumReducer extends Reducer<Text,IntWritable,Text,IntWritable> {
                        private IntWritable result = new IntWritable();
                        public void reduce(Text key, Iterable<IntWritable> values,Context context) throws IOException, InterruptedException {
                          int sum = 0;
                          for (IntWritable val : values) {
                            sum += val.get();
                          }
                          result.set(sum);
                          context.write(key, result);
                        }
                      }

                      public static void main(String[] args) throws Exception {
                        Configuration conf = new Configuration();
                        Job job = Job.getInstance(conf, "word count");
                        job.setJarByClass(WordCount.class);
                        job.setMapperClass(TokenizerMapper.class);
                        job.setCombinerClass(IntSumReducer.class);
                        job.setReducerClass(IntSumReducer.class);
                        job.setOutputKeyClass(Text.class);
                        job.setOutputValueClass(IntWritable.class);
                        FileInputFormat.addInputPath(job, new Path(args[0]));
                        FileOutputFormat.setOutputPath(job, new Path(args[1]));
                        System.exit(job.waitForCompletion(true) ? 0 : 1);
                      }
                    }
    5.YARN:
        5.1 ç»“æ„ä½“ç³»ï¼š
                å°†èµ„æºç®¡ç†å’Œä½œä¸šè°ƒåº¦/ç›‘è§†çš„åŠŸèƒ½åˆ’åˆ†ä¸ºå•ç‹¬çš„å®ˆæŠ¤è¿›ç¨‹ã€‚
                å…¶æ€æƒ³æ˜¯æ‹¥æœ‰ä¸€ä¸ªå…¨å±€çš„ResourceManager (RM)å’Œæ¯ä¸ªåº”ç”¨ç¨‹åºçš„ApplicationMaster (AM)ã€‚
                åº”ç”¨ç¨‹åºå¯ä»¥æ˜¯å•ä¸ªä½œä¸šï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ç»„ä½œä¸š
        5.2 å†™YARNåº”ç”¨ç¨‹åºï¼š
                 YarnClient yarnClient = YarnClient.createYarnClient();
                 yarnClient.init(conf);
                 yarnClient.start();

                 YarnClientApplication app = yarnClient.createApplication();
                 GetNewApplicationResponse appResponse = app.getNewApplicationResponse();
    6. Hadoop Auth:
        6.1 æ¦‚è¿°ï¼šæ˜¯ä¸€ä¸ªjavaåº“
                URL url = new URL("http://localhost:8080/hadoop-auth/kerberos/who");
                AuthenticatedURL.Token token = new AuthenticatedURL.Token();
                HttpURLConnection conn = new AuthenticatedURL().openConnection(url, token);
                conn = new AuthenticatedURL().openConnection(url, token);