1.å­¦ä¹ SparkçŸ¥è¯†ç‚¹ï¼š
    1.æ¦‚å¿µï¼šæ˜¯ä¸€ä¸ªç”¨äºå¤§è§„æ¨¡æ•°æ®å¤„ç†çš„ç»Ÿä¸€åˆ†æå¼•æ“ã€‚æ˜¯é›†ç¾¤è®¡ç®—ç³»ç»Ÿã€‚
    2.æ¶æ„ï¼šApache Spark:
                Spark SQL
                Spark Streaming
                MLib(machine learning)
                Graphx(graph)
    3.javaä¾‹å­ï¼šç»Ÿè®¡å­—æ¯çš„æ•°å­—ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿
        import org.apache.spark.sql.SparkSession;
        import org.apache.spark.sql.Dataset;

        public class SimpleApp {
          public static void main(String[] args) {
            String logFile = "YOUR_SPARK_HOME/README.md";
            SparkSession spark = SparkSession.builder().appName("Simple Application").getOrCreate();
            Dataset<String> logData = spark.read().textFile(logFile).cache();

            long numAs = logData.filter(s -> s.contains("a")).count();
            long numBs = logData.filter(s -> s.contains("b")).count();

            System.out.println("Lines with a: " + numAs + ", lines with b: " + numBs);

            spark.stop();
          }
        }
    4.jaråŒ…ä¾èµ–ï¼š
        <dependency>
          <groupId>org.apache.spark</groupId>
          <artifactId>spark-sql_2.12</artifactId>
          <version>2.4.4</version>
          <scope>provided</scope>
        </dependency>
    5.ä½¿ç”¨å‘½ä»¤è¡Œshellè¿è¡Œï¼š
        ./bin/run-example SparkPi
    6.åˆå§‹åŒ–Spark:ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿
        SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);
        JavaSparkContext sc = new JavaSparkContext(conf);
    7.å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼šRDDsğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿
        RDDæ˜¯ä¸€ç»„å¯ä»¥å¹¶è¡Œæ“ä½œçš„å…ƒç´ çš„å®¹é”™é›†åˆã€‚ä¸€æ—¦åˆ›å»ºäº†åˆ†å¸ƒå¼æ•°æ®é›†(distData),å°±å¯ä»¥å¹¶è¡Œåœ°æ“ä½œå®ƒ
        List<Integer> data = Arrays.asList(1, 2, 3, 4, 5);
        JavaRDD<Integer> distData = sc.parallelize(data);
        JavaRDD<String> distFile = sc.textFile("data.txt");

        åŸºç¡€ä»£ç ï¼š
             JavaRDD<String> lines = sc.textFile("data.txt");
             JavaRDD<Integer> lineLengths = lines.map(s -> s.length());
             int totalLength = lineLengths.reduce((a, b) -> a + b);
             //æŒä¹…åŒ–æ•°æ®
             lineLengths.persist(StorageLevel.MEMORY_ONLY());
        ä¼ é€’å‡½æ•°åˆ°Spark:
            class GetLength implements Function<String, Integer> {
              public Integer call(String s) { return s.length(); }
            }
            class Sum implements Function2<Integer, Integer, Integer> {
              public Integer call(Integer a, Integer b) { return a + b; }
            }

            JavaRDD<String> lines = sc.textFile("data.txt");
            JavaRDD<Integer> lineLengths = lines.map(new GetLength());
            int totalLength = lineLengths.reduce(new Sum());
        é”®å€¼å¯¹å¤„ç†ï¼š
            JavaRDD<String> lines = sc.textFile("data.txt");
            JavaPairRDD<String, Integer> pairs = lines.mapToPair(s -> new Tuple2(s, 1));
            JavaPairRDD<String, Integer> counts = pairs.reduceByKey((a, b) -> a + b);
    8.Spark SQLçš„å­¦ä¹ ï¼šã€ç±»ä¼¼äºjava jdbcçš„ä»£ç å­¦ä¹ ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ã€‘
        å¯åŠ¨SparkSession:
            import org.apache.spark.sql.SparkSession;
            SparkSession spark = SparkSession
              .builder()
              .appName("Java Spark SQL basic example")
              .config("spark.some.config.option", "some-value")
              .getOrCreate();
        åˆ›å»ºDataFrames:
            Dataset<Row> df = spark.read().json("examples/src/main/resources/people.json");
            df.show();
        éç±»å‹åŒ–æ•°æ®é›†æ“ä½œ(å³DataFrameæ“ä½œ):
            df.printSchema();
            df.select("name").show();
            df.select(col("name"), col("age").plus(1)).show();
            df.filter(col("age").gt(21)).show();
            df.groupBy("age").count().show();
        ä»¥ç¼–ç¨‹æ–¹å¼è¿è¡ŒSQLæŸ¥è¯¢ï¼š
            df.createOrReplaceTempView("people");
            Dataset<Row> sqlDF = spark.sql("SELECT * FROM people");
            sqlDF.show();
        å…¨å±€ä¸´æ—¶è§†å›¾ï¼š
            df.createGlobalTempView("people");
            spark.sql("SELECT * FROM global_temp.people").show();
            spark.newSession().sql("SELECT * FROM global_temp.people").show();
        åˆ›å»ºæ•°æ®é›†ï¼šæ•°æ®é›†ä½¿ç”¨ä¸“é—¨çš„ç¼–ç å™¨æ¥åºåˆ—åŒ–å¯¹è±¡è¿›è¡Œç½‘ç»œä¼ è¾“
            Encoder<Person> personEncoder = Encoders.bean(Person.class);
            Dataset<Person> javaBeanDS = spark.createDataset(
              Collections.singletonList(person),
              personEncoder
            );
            javaBeanDS.show();

            Encoder<Integer> integerEncoder = Encoders.INT();
            Dataset<Integer> primitiveDS = spark.createDataset(Arrays.asList(1, 2, 3), integerEncoder);
            Dataset<Integer> transformedDS = primitiveDS.map(
                (MapFunction<Integer, Integer>) value -> value + 1,
                integerEncoder);
            transformedDS.collect();

            String path = "examples/src/main/resources/people.json";
            Dataset<Person> peopleDS = spark.read().json(path).as(personEncoder);
            peopleDS.show();

            æœªå®Œå¾…ç»­......âŒ›ï¸âŒ›ï¸âŒ›ï¸âŒ›ï¸âŒ›ï¸
    9.ç»“æ„æµï¼ˆStructured Streamingï¼‰ï¼šğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿
        ç»“æ„åŒ–æµæ˜¯ä¸€ä¸ªå¯ä¼¸ç¼©çš„ã€å®¹é”™çš„æµå¤„ç†å¼•æ“ï¼Œæ„å»ºåœ¨Spark SQLå¼•æ“ä¸Š
        ä»£ç ä¸¾ä¾‹ï¼š
          SparkSession spark = SparkSession.builder().appName("JavaStructuredNetworkWordCount").getOrCreate();
          Dataset<Row> lines = spark.readStream().format("socket").option("host", "localhost")
            .option("port", 9999)
            .load();
          Dataset<String> words = lines.as(Encoders.STRING())
            .flatMap((FlatMapFunction<String, String>) x -> Arrays.asList(x.split(" ")).iterator(), Encoders.STRING());
          Dataset<Row> wordCounts = words.groupBy("value").count();

          è¿è¡Œå°è£…çš„å®ä¾‹ï¼š
              $ nc -lk 9999
              $ ./bin/run-example org.apache.spark.examples.sql.streaming.JavaStructuredNetworkWordCount localhost 9999

          æœªå®Œå¾…ç»­......âŒ›ï¸âŒ›ï¸âŒ›ï¸âŒ›ï¸âŒ›ï¸
    10.Spark Stream:ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿
        Sparkæµæ˜¯æ ¸å¿ƒSpark APIçš„æ‰©å±•ï¼Œæ”¯æŒå¯ä¼¸ç¼©ã€é«˜ååé‡ã€å®¹é”™çš„å®æ—¶æ•°æ®æµå¤„ç†ã€‚
        è¾“å…¥æ˜¯å„ç§æ•°æ®æºï¼šæ¯”å¦‚kafka,Flume,HDFS/S3,Kinesis,Twitter
        å¤„ç†æ˜¯å„ç§é«˜çº§ç®—æ³•ï¼šæ¯”å¦‚mapã€reduceã€join,window
        è¾“å‡ºæ˜¯å„ç§æ•°æ®åº“ï¼šæ¯”å¦‚HDFS,Databases,Dashboards
        ä»£ç ä¸¾ä¾‹ï¼š
            SparkConf conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount");
            JavaStreamingContext jssc = new JavaStreamingContext(conf, Durations.seconds(1));
            JavaReceiverInputDStream<String> lines = jssc.socketTextStream("localhost", 9999);
            JavaDStream<String> words = lines.flatMap(x -> Arrays.asList(x.split(" ")).iterator());
            JavaPairDStream<String, Integer> pairs = words.mapToPair(s -> new Tuple2<>(s, 1));
            JavaPairDStream<String, Integer> wordCounts = pairs.reduceByKey((i1, i2) -> i1 + i2);
            wordCounts.print();
            jssc.start();
            jssc.awaitTermination();
        è¿è¡Œå°è£…çš„å®ä¾‹ï¼š
            $ nc -lk 9999
            $ ./bin/run-example streaming.JavaNetworkWordCount localhost 9999
        ä¾èµ–jaråŒ…ï¼šå¹¶ä¸”æ˜¯ä¼ é€’ä¾èµ–
            <dependency>
                <groupId>org.apache.spark</groupId>
                <artifactId>spark-streaming_2.12</artifactId>
                <version>2.4.4</version>
                <scope>provided</scope>
            </dependency>

            æ•°æ®æºçš„ä¼ é€’ä¾èµ–ï¼š
                spark-streaming-kafka-0-10_2.12
                spark-streaming-flume_2.12
                spark-streaming-kinesis-asl_2.12 [Amazon Software License]
        æœªå®Œå¾…ç»­......âŒ›ï¸âŒ›ï¸âŒ›ï¸âŒ›ï¸âŒ›ï¸
    11.MLib(æœºå™¨å­¦ä¹ åº“):ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿
        åˆ†ä¸ºæ ¸å¿ƒåº“å’ŒåŸºäºRDDçš„åº“ï¼Œä¸»è¦æ˜¯å„ç§ä»£ç æ“ä½œ
            List<Row> data = Arrays.asList(
              RowFactory.create(Vectors.sparse(4, new int[]{0, 3}, new double[]{1.0, -2.0})),
              RowFactory.create(Vectors.dense(4.0, 5.0, 0.0, 3.0)),
              RowFactory.create(Vectors.dense(6.0, 7.0, 0.0, 8.0)),
              RowFactory.create(Vectors.sparse(4, new int[]{0, 3}, new double[]{9.0, 1.0}))
            );

            StructType schema = new StructType(new StructField[]{
              new StructField("features", new VectorUDT(), false, Metadata.empty()),
            });

            Dataset<Row> df = spark.createDataFrame(data, schema);
            Row r1 = Correlation.corr(df, "features").head();
            System.out.println("Pearson correlation matrix:\n" + r1.get(0).toString());

            Row r2 = Correlation.corr(df, "features", "spearman").head();
            System.out.println("Spearman correlation matrix:\n" + r2.get(0).toString());
        æœªå®Œå¾…ç»­......âŒ›ï¸âŒ›ï¸âŒ›ï¸âŒ›ï¸âŒ›ï¸
    12.GraphX:ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿
        GraphXæ˜¯Sparkä¸­å›¾å½¢å’Œå›¾å½¢å¹¶è¡Œè®¡ç®—çš„æ–°ç»„ä»¶ã€‚ã€è¿™æ˜¯é«˜çº§åŠŸèƒ½ã€‘
        jaråŒ…ä¾èµ–ï¼š
            import org.apache.spark._
            import org.apache.spark.graphx._
            import org.apache.spark.rdd.RDD
        æœªå®Œå¾…ç»­......âŒ›ï¸âŒ›ï¸âŒ›ï¸âŒ›ï¸âŒ›ï¸
    13.SparkR:ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿
        SparkRæ˜¯ä¸€ä¸ªRåŒ…ï¼Œæä¾›äº†ä¸€ä¸ªè½»é‡çº§å‰ç«¯ï¼Œé€šè¿‡Rè¯­è¨€è°ƒç”¨Apache Sparkã€‚SparkRè¿˜æ”¯æŒä½¿ç”¨MLlibçš„åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ 
        å¯åŠ¨ï¼šsparkR.session()
        æœªå®Œå¾…ç»­......âŒ›ï¸âŒ›ï¸âŒ›ï¸âŒ›ï¸âŒ›ï¸
    14.éƒ¨ç½²ï¼šğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿
        å¯åŠ¨é›†ç¾¤ï¼š./sbin/start-master.sh  ç„¶åé€šè¿‡http://localhost:8080è®¿é—®å¯åŠ¨çš„webæœåŠ¡å™¨
        å¯åŠ¨workerè¿æ¥åˆ°masteræœåŠ¡å™¨ï¼š./sbin/start-slave.sh <master-spark-URL>
        å°†åº”ç”¨è¿æ¥åˆ°é›†ç¾¤ï¼š./bin/spark-class org.apache.spark.deploy.Client kill <master url> <driver ID>
    15.å…¶ä»–æ“ä½œï¼š
         Spark propertiesé…ç½®ï¼šã€å¯ä»¥åŠ¨æ€åŠ è½½ã€‘
            val conf = new SparkConf().setMaster("local[2]").setAppName("CountingSheep");
            val sc = new SparkContext(conf);

            val sc = new SparkContext(new SparkConf())  //åŠ¨æ€åŠ è½½
         ç›‘æ§ï¼š
            ./sbin/start-history-server.sh
            ç„¶åè®¿é—®ï¼šhttp://<server-url>:18080





